"""
–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
"""
import asyncio
import os
from datetime import datetime, timedelta
from typing import Optional

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.triggers.cron import CronTrigger
from loguru import logger

from app.scraper.scraper import FTScraper, ArticleProcessor
from app.db.database import engine
from app.models.models import Base


class ScrapingScheduler:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞"""

    def __init__(self):
        self.scheduler = AsyncIOScheduler()
        self.scraper = None
        self.processor = ArticleProcessor()
        self.is_initial_run = True

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.interval_hours = int(os.getenv('SCRAPER_INTERVAL_HOURS', '1'))
        self.initial_days_back = int(os.getenv('INITIAL_DAYS_BACK', '30'))
        self.max_concurrent = int(os.getenv('MAX_CONCURRENT_REQUESTS', '5'))
        self.request_delay = int(os.getenv('REQUEST_DELAY', '2'))

    async def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö - —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        logger.info("üóÑÔ∏è Initializing database tables...")
        try:
            async with engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)
            logger.success("‚úÖ Database tables created successfully")
        except Exception as e:
            logger.error(f"‚ùå Error creating database tables: {str(e)}")
            raise

    async def run_scraping_job(self, days_back: Optional[int] = None):
        """
        –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞

        Args:
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π (None = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ)
        """
        start_time = datetime.utcnow()

        if days_back is None:
            days_back = self.initial_days_back if self.is_initial_run else 1

        run_type = "initial" if self.is_initial_run else "hourly"

        logger.info(f"üöÄ Starting {run_type} scraping job (last {days_back} days)")

        stats = {
            'found': 0,
            'scraped': 0,
            'saved': 0,
            'skipped': 0,
            'errors': 0
        }

        try:
            async with FTScraper() as scraper:
                # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
                article_links = await scraper.get_article_links(days_back=days_back)
                stats['found'] = len(article_links)

                if not article_links:
                    logger.warning("‚ö†Ô∏è No articles found")
                    return stats

                # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                semaphore = asyncio.Semaphore(self.max_concurrent)

                # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏
                async def process_article(article_info):
                    async with semaphore:
                        try:
                            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                            await asyncio.sleep(self.request_delay)

                            # –°–∫—Ä–∞–ø–∏–º —Å—Ç–∞—Ç—å—é
                            article_data = await scraper.scrape_article(article_info['url'])

                            if article_data is None:
                                stats['skipped'] += 1
                                return

                            stats['scraped'] += 1

                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
                            saved = await self.processor.save_article(article_data)
                            if saved:
                                stats['saved'] += 1
                            else:
                                stats['skipped'] += 1

                        except Exception as e:
                            logger.error(f"‚ùå Error processing article {article_info['url']}: {str(e)}")
                            stats['errors'] += 1

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                tasks = [process_article(article_info) for article_info in article_links]
                await asyncio.gather(*tasks, return_exceptions=True)

        except Exception as e:
            logger.error(f"‚ùå Critical error in scraping job: {str(e)}")
            stats['errors'] += 1
            raise

        finally:
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            duration = (datetime.utcnow() - start_time).total_seconds()

            logger.info(
                f"üìä Scraping completed in {duration:.1f}s: "
                f"Found: {stats['found']}, "
                f"Scraped: {stats['scraped']}, "
                f"Saved: {stats['saved']}, "
                f"Skipped: {stats['skipped']}, "
                f"Errors: {stats['errors']}"
            )

            # –ü–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
            if self.is_initial_run:
                self.is_initial_run = False
                logger.info("üîÑ Switched to hourly scraping mode")

        return stats

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            await self.init_database()

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ä–∞–∑—É
            logger.info("üéØ Running initial scraping...")
            await self.run_scraping_job()

            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏
            self.scheduler.add_job(
                self.run_scraping_job,
                trigger=IntervalTrigger(hours=self.interval_hours),
                id='hourly_scraping',
                name='Hourly FT Scraping',
                max_instances=1,
                coalesce=True,
                misfire_grace_time=300  # 5 –º–∏–Ω—É—Ç
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            self.scheduler.add_job(
                self._cleanup_logs,
                trigger=CronTrigger(hour=3),  # –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 3 —É—Ç—Ä–∞
                id='cleanup_logs',
                name='Daily log cleanup'
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
            self.scheduler.start()
            logger.success(f"‚è∞ Scheduler started with {self.interval_hours}h interval")

            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–±–æ—Ç—É
            try:
                while True:
                    await asyncio.sleep(60)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É

            except KeyboardInterrupt:
                logger.info("‚èπÔ∏è Received shutdown signal")

        except Exception as e:
            logger.error(f"‚ùå Error starting scheduler: {str(e)}")
            raise

        finally:
            if self.scheduler.running:
                self.scheduler.shutdown(wait=True)
                logger.info("üõë Scheduler stopped")

    async def _cleanup_logs(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"""
        logger.info("üßπ Running daily log cleanup...")
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
        # –ù–∞–ø—Ä–∏–º–µ—Ä, —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤ —Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        if self.scheduler.running:
            self.scheduler.shutdown(wait=True)
            logger.info("üõë Scheduler stopped manually")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
async def run_manual_scraping(days_back: int = 1):
    """
    –†—É—á–Ω–æ–π –∑–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

    Args:
        days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥
    """
    scheduler = ScrapingScheduler()
    await scheduler.init_database()
    return await scheduler.run_scraping_job(days_back=days_back)