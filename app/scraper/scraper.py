"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ Financial Times
"""
import asyncio
import datetime
from typing import Optional, List, Dict, Any
from urllib.parse import urljoin

from playwright.async_api import async_playwright, Page, Browser, ViewportSize
from bs4 import BeautifulSoup
from loguru import logger
from sqlalchemy.exc import IntegrityError

from app.db.database import get_session
from app.models.models import Article
from sqlalchemy import select, func


class FTScraper:
    """–°–∫—Ä–∞–ø–µ—Ä –¥–ª—è Financial Times —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Å—Å–∏–∏"""

    def __init__(self):
        self.base_url = "https://www.ft.com"
        self.world_url = "https://www.ft.com/world"
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    async def init_browser(self, max_retries: int = 3) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(max_retries):
            try:
                playwright = await async_playwright().start()
                self.browser = await playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox', 
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-extensions',
                        '--no-first-run'
                    ]
                )
                context = await self.browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    viewport=ViewportSize(width=1920, height=1080)
                )
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã
                context.set_default_timeout(30000)  # 30 —Å–µ–∫—É–Ω–¥
                context.set_default_navigation_timeout(30000)
                
                self.page = await context.new_page()
                logger.info("üåê –ë—Ä–∞—É–∑–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                return
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞ –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    raise

    async def close_browser(self) -> None:
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            if self.browser:
                await self.browser.close()
                logger.info("üî¥ –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –±—Ä–∞—É–∑–µ—Ä–∞: {e}")

    @staticmethod
    async def is_first_run() -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—É—Å–∫ –ø–µ—Ä–≤—ã–º (–Ω–µ—Ç —Å—Ç–∞—Ç–µ–π –≤ –±–∞–∑–µ)"""
        try:
            async for session in get_session():
                result = await session.execute(select(func.count(Article.id)))
                count = result.scalar()
                is_first = count == 0
                logger.info(f"üìä –°—Ç–∞—Ç–µ–π –≤ –±–∞–∑–µ: {count}, –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫: {is_first}")
                return is_first
            return True
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞: {e}")
            return True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–º –∑–∞–ø—É—Å–∫–æ–º

    @staticmethod
    def _is_article_recent(published_at: datetime.datetime, hours_limit: int = 1) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –Ω–µ–¥–∞–≤–Ω–µ–π (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —á–∞—Å–æ–≤)"""
        now = datetime.datetime.now(datetime.timezone.utc)
        time_limit = now - datetime.timedelta(hours=hours_limit)
        return published_at >= time_limit

    @staticmethod
    def _is_article_within_days(published_at: datetime.datetime, days_limit: int = 30) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π"""
        now = datetime.datetime.now(datetime.timezone.utc)
        time_limit = now - datetime.timedelta(days=days_limit)
        return published_at >= time_limit

    @staticmethod
    def _parse_publish_date(date_str: str) -> datetime.datetime:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ title"""
        try:
            date_obj = datetime.datetime.strptime(date_str, "%B %d %Y %I:%M %p")
            return date_obj.replace(tzinfo=datetime.timezone.utc)
        except ValueError:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str}")
            return datetime.datetime.now(datetime.timezone.utc)

    def _extract_article_data(self, article_element, time_filter_func=None) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–º–∏—É–º
            premium_label = article_element.find('span', class_='o-labels--premium')
            if premium_label:
                logger.debug("‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–º–∏—É–º —Å—Ç–∞—Ç—å—é")
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–∞ (–∫–∞—Ç–µ–≥–æ—Ä–∏—é)
            author_element = article_element.find('a', class_='o-teaser__tag')
            author = author_element.get_text(strip=True) if author_element else "Unknown"

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ URL
            title_element = article_element.find('a', class_='js-teaser-heading-link')
            if not title_element:
                logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏")
                return None
            
            title = title_element.get_text(strip=True)
            relative_url = title_element.get('href')
            full_url = urljoin(self.base_url, relative_url)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            standfirst_element = article_element.find('a', class_='js-teaser-standfirst-link')
            content = standfirst_element.get_text(strip=True) if standfirst_element else ""

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            time_element = article_element.find('time')
            if time_element and time_element.get('title'):
                published_at = self._parse_publish_date(time_element.get('title'))
            else:
                published_at = datetime.datetime.now(datetime.timezone.utc)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω
            if time_filter_func and not time_filter_func(published_at):
                logger.debug(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å—é –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {title[:50]}...")
                return None

            return {
                'url': full_url,
                'title': title,
                'content': content,
                'author': author,
                'published_at': published_at,
                'scraped_at': datetime.datetime.now(datetime.timezone.utc)
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏: {e}")
            return None

    async def scrape_single_page(self, page_num: int = 1, time_filter_func=None, max_retries: int = 3) -> List[Dict[str, Any]]:
        """–°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç–µ–π —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(max_retries):
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                if page_num == 1:
                    url = self.world_url
                else:
                    url = f"{self.world_url}?page={page_num}"
                
                logger.info(f"üìÑ –°–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {url}")
                
                # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–∞–π–º–∞—É—Ç–æ–≤
                try:
                    await self.page.goto(url, wait_until='networkidle', timeout=30000)
                except Exception as nav_error:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num}: {nav_error}")
                    # –ü—Ä–æ–±—É–µ–º –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è networkidle
                    await self.page.goto(url, wait_until='load', timeout=30000)
                
                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                await asyncio.sleep(2)
                
                # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π
                try:
                    await self.page.wait_for_selector('ul.o-teaser-collection__list', timeout=10000)
                except TimeoutError:
                    logger.warning(f"‚ö†Ô∏è –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")

                # –ü–æ–ª—É—á–∞–µ–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                content = await self.page.content()
                soup = BeautifulSoup(content, 'html.parser')

                # –ù–∞—Ö–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π
                articles_list = soup.find('ul', class_='o-teaser-collection__list')
                if not articles_list:
                    logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                    return []

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å—Ç–∞—Ç–µ–π
                article_items = articles_list.find_all('li', class_='o-teaser-collection__item')
                logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(article_items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")

                articles_data = []
                for item in article_items:
                    try:
                        article_data = self._extract_article_data(item, time_filter_func)
                        if article_data:
                            articles_data.append(article_data)
                    except Exception as extract_error:
                        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç—å–∏: {extract_error}")
                        continue

                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(articles_data)} —Å—Ç–∞—Ç–µ–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
                return articles_data

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
                if attempt < max_retries - 1:
                    # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
                    delay = min(2 ** attempt, 10)  # –ú–∞–∫—Å–∏–º—É–º 10 —Å–µ–∫—É–Ω–¥
                    logger.info(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫—Ä–∞–ø–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_num} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                    
        return []  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã

    async def scrape_articles_with_pagination(self, max_pages: int = 10, time_filter_func=None) -> List[Dict[str, Any]]:
        """–°–∫—Ä–∞–ø–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        try:
            logger.info(f"üìö –ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π (–º–∞–∫—Å. {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)...")
            
            all_articles = []
            no_articles_count = 0
            page_num = 0
            
            for page_num in range(1, max_pages + 1):
                page_articles = await self.scrape_single_page(page_num, time_filter_func)
                
                if not page_articles:
                    no_articles_count += 1
                    logger.warning(f"‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç–∞—Ç–µ–π")
                    
                    # –ï—Å–ª–∏ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥ –±–µ–∑ —Å—Ç–∞—Ç–µ–π - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                    if no_articles_count >= 3:
                        logger.info("üõë –ù–∞–π–¥–µ–Ω–æ 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–¥—Ä—è–¥ –±–µ–∑ —Å—Ç–∞—Ç–µ–π, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥")
                        break
                else:
                    no_articles_count = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
                    all_articles.extend(page_articles)
                    
                    # –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç–∞—Ç—å—é
                    if time_filter_func and page_articles:
                        last_article_date = page_articles[-1]['published_at']
                        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç–∞—Ç—å—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä–∞—è, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                        if not time_filter_func(last_article_date):
                            logger.info(f"üïê –î–æ—Å—Ç–∏–≥–Ω—É—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–º–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥")
                            break
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                await asyncio.sleep(1)
            
            logger.info(f"üéâ –ó–∞–≤–µ—Ä—à–µ–Ω —Å–∫—Ä–∞–ø–∏–Ω–≥ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π: —Å–æ–±—Ä–∞–Ω–æ {len(all_articles)} —Å—Ç–∞—Ç–µ–π —Å {page_num} —Å—Ç—Ä–∞–Ω–∏—Ü")
            return all_articles

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π: {e}")
            return []

    async def scrape_articles_list(self, time_filter_func=None) -> List[Dict[str, Any]]:
        """–°–∫—Ä–∞–ø–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Å—Ç–∞—Ç–µ–π —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º–∏—Ä–∞ (–±–µ–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏)"""
        return await self.scrape_single_page(1, time_filter_func)

    @staticmethod
    async def save_articles_to_db(articles_data: List[Dict[str, Any]], max_retries: int = 3) -> int:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        saved_count = 0
        failed_count = 0
        
        if not articles_data:
            logger.info("üìù –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return 0
        
        for attempt in range(max_retries):
            try:
                async for session in get_session():
                    batch_saved = 0
                    
                    for i, article_data in enumerate(articles_data):
                        try:
                            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
                            if not all(key in article_data for key in ['url', 'title', 'content']):
                                logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ —Å—Ç–∞—Ç—å—è —Å –Ω–µ–ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {article_data.get('title', 'Unknown')}")
                                continue
                            
                            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Å—Ç–∞—Ç—å–∏
                            article = Article(**article_data)
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–µ—Å—Å–∏—é
                            session.add(article)
                            await session.commit()
                            batch_saved += 1
                            saved_count += 1
                            logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {article.title[:50]}...")
                            
                        except IntegrityError:
                            # –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ URL)
                            await session.rollback()
                            logger.debug(f"‚è≠Ô∏è –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {article_data.get('title', 'Unknown')[:50]}...")
                            continue
                            
                        except Exception as e:
                            await session.rollback()
                            failed_count += 1
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ {i+1}: {e}")
                            
                            # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –ø–æ–¥—Ä—è–¥, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                            if failed_count > 5:
                                logger.error("‚ùå –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é")
                                break
                            continue
                    
                    logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω batch: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {batch_saved} —Å—Ç–∞—Ç–µ–π")
                    break  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª–∏, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø–æ–ø—ã—Ç–æ–∫
                    
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –ë–î –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
        
        logger.info(f"‚úÖ –ò—Ç–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
        if failed_count > 0:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {failed_count} —Å—Ç–∞—Ç–µ–π –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫")
            
        return saved_count

    async def run_scraping(self) -> None:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–∂–∏–º–∞"""
        try:
            logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ Financial Times...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±—Ä–∞—É–∑–µ—Ä
            await self.init_browser()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
            is_first = await self.is_first_run()
            
            if is_first:
                # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
                logger.info("üÜï –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π...")
                time_filter = lambda date: self._is_article_within_days(date, 30)
                articles_data = await self.scrape_articles_with_pagination(
                    max_pages=100,
                    time_filter_func=time_filter
                )
            else:
                # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ - —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
                logger.info("‚è∞ –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ - —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç—å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å...")
                time_filter = lambda date: self._is_article_recent(date, 1)
                articles_data = await self.scrape_articles_with_pagination(
                    max_pages=5,  # –ú–∞–∫—Å–∏–º—É–º 5 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–±–æ—Ä–∞ –∑–∞ —á–∞—Å
                    time_filter_func=time_filter
                )
            
            if articles_data:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                saved_count = await self.save_articles_to_db(articles_data)
                logger.info(f"üéâ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(articles_data)}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_count}")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç–µ–π")

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
            await self.close_browser()

    async def run_initial_scraping(self) -> None:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π –∑–∞ 30 –¥–Ω–µ–π (–¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞)"""
        try:
            logger.info("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä —Å—Ç–∞—Ç–µ–π –∑–∞ 30 –¥–Ω–µ–π...")
            
            await self.init_browser()
            
            time_filter = lambda date: self._is_article_within_days(date, 30)
            articles_data = await self.scrape_articles_with_pagination(
                max_pages=50,
                time_filter_func=time_filter
            )
            
            if articles_data:
                saved_count = await self.save_articles_to_db(articles_data)
                logger.info(f"üéâ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(articles_data)}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_count}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–±–æ—Ä–∞: {e}")
        finally:
            await self.close_browser()

    async def run_hourly_scraping(self) -> None:
        """–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å"""
        try:
            logger.info("‚è±Ô∏è –°–±–æ—Ä –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å...")
            
            await self.init_browser()
            
            time_filter = lambda date: self._is_article_recent(date, 1)
            articles_data = await self.scrape_articles_with_pagination(
                max_pages=5,
                time_filter_func=time_filter
            )
            
            if articles_data:
                saved_count = await self.save_articles_to_db(articles_data)
                logger.info(f"üéâ –ü–æ—á–∞—Å–æ–≤–æ–π —Å–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(articles_data)}, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {saved_count}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—á–∞—Å–æ–≤–æ–≥–æ —Å–±–æ—Ä–∞: {e}")
        finally:
            await self.close_browser()